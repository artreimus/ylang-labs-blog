---
title: 'DSPy: Beyond Prompts - A Better Way to Work with LLMs'
date: '2025-01-01'
tags: ['AI/ML', 'Generative-AI']
draft: true
summary: 'DSPy is a framework for building LLM applications that goes beyond traditional prompt engineering. It provides a programmatic approach to working with LLMs, allowing developers to build more robust, maintainable, and scalable applications.'
authors: ['arthur-reimus', 'ezekiel-mariano', 'van-panugan']
cardImage: '/static/images/blogs/dspy-beyond-prompts-a-better-way-to-work-with-llms/cardImage.png'
images: ['/static/images/blogs/dspy-beyond-prompts-a-better-way-to-work-with-llms/blogHeader.png']
layout: 'PostLayout'
---

DSPy represents a revolutionary shift in how developers work with Large Language Models (LLMs). Instead of grappling with brittle prompts and manual optimizations, DSPy offers a programmatic framework that makes LLM development more robust, maintainable, and scalable.

This guide will walk you through:

- **Why traditional prompt engineering falls short for complex applications**
- **How DSPy transforms LLM development with programmatic abstractions**
- **Key concepts and patterns for building reliable AI systems**
- **Best practices for developing production-ready DSPy applications**

By the end, you'll understand why DSPy is becoming the preferred choice for developers building sophisticated LLM-powered applications.

## 1. Introduction to DSPy

### 1.1 The Evolution from Prompting to Programming

Imagine spending hours crafting the perfect prompt for your AI application, only to have it break with a minor change. This frustration is common in traditional prompt engineering and is one of the key issues DSPy aims to solve.

### The Limitations of Traditional Prompting

Traditional prompt engineering, while powerful, faces several critical challenges when building complex AI applications:

- **Brittle Solutions**: Even small prompt changes can lead to unexpected behavior.

  ```python
  # Traditional approach - brittle and hard to maintain
  prompt = """Given a sentence, determine if it's positive or negative.
  Examples:
  'I love this!' -> positive
  'This is terrible' -> negative

  Now analyze: '{input_text}'"""
  ```

- **Poor Maintainability**: Long prompts become unwieldy and difficult to debug.
- **Limited Composability**: Combining multiple prompts in a structured way is challenging.
- **Manual Optimization**: Requires constant hand-tuning of prompts.
- **Lack of Systematic Testing**: Difficult to evaluate and validate prompt effectiveness.

### The DSPy Paradigm Shift

DSPy introduces a fundamentally different approach: treating LLM interactions as programmatic components rather than string prompts. Here's how it transforms the development process:

1. **Declarative Programming**

   ```python
   # DSPy approach - clean, maintainable, and optimizable
   class SentimentAnalyzer(dspy.Signature):
       """Analyze the sentiment of a sentence."""
       text: str = dspy.InputField()
       sentiment: Literal['positive', 'negative'] = dspy.OutputField()
   ```

   - **Define _what_ you want the LLM to do, not _how_ to do it.**
   - **Use Python classes and methods instead of raw prompt strings.**
   - **Leverage type hints and structured signatures for clarity.**

2. **Modular Design**

   ```python
   # Compose multiple modules for complex tasks
   class ReviewAnalyzer(dspy.Module):
       def __init__(self):
           self.sentiment = dspy.ChainOfThought(SentimentAnalyzer)
           self.summarize = dspy.ChainOfThought('review -> key_points: list[str]')
   ```

   - **Break complex tasks into smaller, reusable components.**
   - **Compose modules to build sophisticated pipelines.**
   - **Swap components without rewriting entire systems.**

3. **Systematic Optimization**

   ```python
   # Automatically optimize your modules
   optimizer = dspy.BootstrapFewShot(metric=accuracy_metric)
   optimized_analyzer = optimizer.compile(sentiment_analyzer, trainset=examples)
   ```

   - **Automatically optimize prompts using training data.**
   - **Measure and improve performance with metrics.**
   - **Apply machine learning principles to prompt development.**

### Real-World Impact

DSPy has empowered organizations and researchers to build more robust and maintainable AI systems. Here are some notable examples:

- **Enterprise Applications**:

  - VMware's research team published _"The Unreasonable Effectiveness of Eccentric Automatic Prompts"_, demonstrating DSPy's effectiveness in production environments.
  - Portkey integrated DSPy to track metrics and optimize performance across 250+ LLM configurations.
  - Weaviate incorporated DSPy into their vector search applications, as documented in their DSPy Cookbook.

- **Research Implementations**:
  - Karel D'Oosterlinck developed IReRa (Sophisticated Extreme Multi-Class Classification).
  - Saiful Haq's work showed improvements in Indian Languages Natural Language Inference.
  - Chris Levy showcased DSPy's capabilities on BIG-Bench Hard tasks.

These implementations highlight DSPy's versatility across different domains, from academic research to production systems. The framework's modular design and systematic optimization approach make it especially valuable for:

- **Building maintainable AI pipelines**
- **Implementing systematic testing and evaluation**
- **Scaling solutions across different model configurations**
- **Enabling reproducible research and development**

Rather than relying on anecdotal improvements, DSPy's impact is best measured by its growing adoption in the open-source community and its use in published research and production systems.

### 1.2 Core DSPy Philosophy

DSPy's philosophy centers around three fundamental principles that guide the development of LLM applications.

#### Declarative Programming with LLMs

Instead of writing explicit prompts, DSPy encourages developers to:

1. **Define Signatures**

   ```python
   class QuestionAnswering(dspy.Signature):
       """Answer questions based on provided context."""
       context: str = dspy.InputField()
       question: str = dspy.InputField()
       answer: str = dspy.OutputField()
       confidence: float = dspy.OutputField()
   ```

   - **Specify input and output types clearly.**
   - **Use Python type hints for better clarity.**
   - **Focus on the "what" rather than the "how."**

2. **Use Modules**

   ```python
   class RAGSystem(dspy.Module):
       def __init__(self):
           self.retriever = dspy.Retrieve(k=3)
           self.generator = dspy.ChainOfThought(QuestionAnswering)
   ```

   - **Leverage pre-built components like `Predict` and `ChainOfThought`.**
   - **Create custom modules for specific tasks.**
   - **Compose modules to build complex workflows.**

#### Separation of Concerns

DSPy enforces a clear separation between:

1. **Signatures vs. Implementation**

   - **Signatures define the interface (inputs/outputs).**
   - **Implementation details are handled by DSPy modules.**
   - **Prompts are generated and optimized automatically.**

2. **Program Logic vs. LLM Interaction**
   - **Business logic remains in standard Python code.**
   - **LLM interactions are encapsulated in DSPy modules.**
   - **Clear boundaries between different system components.**

#### Building Maintainable Systems

DSPy promotes practices that lead to more maintainable AI systems:

1. **Testability**

   - **Define clear metrics for evaluation.**
   - **Create comprehensive test sets.**
   - **Automate testing of system components.**

2. **Modularity**

   - **Break down complex tasks into smaller modules.**
   - **Reuse components across different parts of the system.**
   - **Easily modify individual components.**

3. **Optimization**
   - **Systematic approach to improving performance.**
   - **Data-driven optimization of prompts.**
   - **Measurable improvements through metrics.**

By adhering to these principles, developers can create AI systems that are not only powerful but also maintainable, testable, and scalable.

### 1.3 DSPy vs Other Frameworks

While frameworks like LangChain and LlamaIndex have gained popularity for building LLM applications, DSPy takes a fundamentally different approach to prompt engineering and system optimization. Let's examine the key differences:

#### Framework Comparison

| Feature               | DSPy                                                                | LangChain                                 | LlamaIndex                                  |
| --------------------- | ------------------------------------------------------------------- | ----------------------------------------- | ------------------------------------------- |
| **Core Philosophy**   | Programmatic LLM control with automatic optimization                | Chain-based prompt orchestration          | Document-centric data structuring           |
| **Prompt Management** | Automatic optimization and generation through compilers             | Manual prompt engineering and templates   | Template-based prompting with manual tuning |
| **Optimization**      | Built-in systematic optimization of prompts and weights             | Manual tuning and chain adjustment        | Manual tuning of retrievers and prompts     |
| **Use Case Strength** | Production systems requiring reliability and continuous improvement | Rapid prototyping and workflow automation | Document retrieval and knowledge bases      |

#### Key Differentiators

1. **Optimization-First Approach**

   - **DSPy automatically optimizes prompts and weights through compiler techniques.**
   - **Enables systematic improvement based on metrics and examples.**
   - **Reduces reliance on manual prompt engineering.**

2. **Production Reliability**
   - **DSPy's programmatic approach leads to more maintainable systems.**
   - **Built-in testing and evaluation capabilities.**
   - **Automatic adaptation to changes in data or requirements.**

By understanding these differences, developers can choose DSPy for building production systems that require reliability and continuous optimization, while considering other frameworks for rapid prototyping or document-centric applications.

## 2. Getting Started with DSPy

### 2.1 Setup and Configuration

Getting started with DSPy involves setting up your development environment and configuring your language model. One of DSPy's key strengths is its LLM-agnostic design, achieved through integration with LiteLLM under the hood. This means you can easily switch between different LLM providers without changing your application code.

#### LLM Provider Support

DSPy, through LiteLLM, supports a wide range of LLM providers:

- **Cloud providers**: OpenAI, Anthropic, Azure
- **Enterprise solutions**: Databricks, watsonx
- **Local deployments**: Via Ollama
- **Self-hosted models**: On GPU servers
- **Many other providers**: Through LiteLLM's standardized interface

This flexibility allows you to:

- **Choose the most suitable provider for your needs.**
- **Switch providers without rewriting code.**
- **Use different models for different components.**
- **Maintain provider independence.**

While this guide uses IBM watsonx as our example provider, you can easily adapt the code to use any other supported LLM provider by simply changing the configuration. Let's proceed with the setup:

1. **Installation**

   First, install DSPy and its dependencies:

   ```bash
   pip install dspy-ai python-dotenv
   ```

2. **Environment Configuration**

   DSPy requires proper authentication to interact with watsonx. Create a `.env` file in your project root with your credentials:

   ```bash
   WATSONX_URL=your_watsonx_url
   WATSONX_APIKEY=your_api_key
   WATSONX_PROJECT_ID=your_project_id
   ```

3. **Basic Configuration**

   Set up DSPy with watsonx as your language model provider:

   ```python
   import dspy
   import os
   from dotenv import load_dotenv

   # Load environment variables
   load_dotenv()

   # Configure WatsonX credentials
   WATSONX_URL = os.getenv("WATSONX_URL")
   WATSONX_APIKEY = os.getenv("WATSONX_APIKEY")
   WATSONX_PROJECT_ID = os.getenv("WATSONX_PROJECT_ID")

   # Set up the language model
   lm = dspy.LM(
       "watsonx/meta-llama/llama-3-8b-instruct",
       project_id=WATSONX_PROJECT_ID,
   )

   # Configure DSPy with the language model
   dspy.configure(lm=lm, trace=[], temperature=0.7)
   ```

### 2.2 Your First DSPy Program

Understanding DSPy's programming model is best done through hands-on experience. Let's create a simple question-answering system:

1. **Define the Signature**

   In DSPy, we start by defining what we want our program to do using a Signature:

   ```python
   class BasicQA(dspy.Signature):
       """Answer questions with short factoid answers."""

       question = dspy.InputField()
       answer = dspy.OutputField(desc="often between 1 and 5 words")
   ```

   This signature tells DSPy that our program takes a question as input and produces an answer as output.

2. **Create and Use the Predictor**

   With our signature defined, we can create a predictor and use it:

   ```python
   # Define the predictor
   generate_answer = dspy.Predict(BasicQA)

   # Test with a sample question
   test_question = "What country was the winner of the Nobel Prize in Literature in 2006 from and what was their name?"

   prediction = generate_answer(question=test_question)
   print(f"Answer: {prediction.answer}")
   ```

### 2.3 Working with Data in DSPy

DSPy uses `Example` objects to handle data consistently across your application:

1. **Creating Examples**

   ```python
   # Single example
   qa_pair = dspy.Example(
       question="Who won the Nobel Prize in Literature in 2006?",
       answer="Orhan Pamuk from Turkey"
   )

   # Creating a dataset
   trainset = [
       dspy.Example(
           question="Who won the Nobel Prize in Literature in 2006?",
           answer="Orhan Pamuk from Turkey"
       ),
       dspy.Example(
           question="What is the capital of France?",
           answer="Paris"
       )
   ]
   ```

2. **Specifying Input Fields**

   ```python
   # Mark which fields are inputs
   qa_pair = qa_pair.with_inputs("question")

   # Access inputs and labels separately
   input_only = qa_pair.inputs()
   label_only = qa_pair.labels()
   ```

### 2.4 Best Practices for Dataset Creation

When building datasets for DSPy applications, consider these key practices:

1. **Diverse Examples**

   - Include a variety of question types.
   - Cover different topics and domains.
   - Include edge cases and common scenarios.

2. **Data Quality**

   - Ensure accurate answers.
   - Maintain consistent formatting.
   - Validate data before use.

3. **Dataset Organization**

   ```python
   # Organize datasets by purpose
   train_examples = [...]  # For training
   dev_examples = [...]    # For development/validation
   test_examples = [...]   # For final testing
   ```

4. **Data Augmentation**
   - Rephrase questions.
   - Create variations of existing examples.
   - Generate synthetic examples when needed.

By following these setup steps and best practices, you'll have a solid foundation for building more complex DSPy applications. The next sections will explore more advanced features and optimization techniques.

## 3. Core DSPy Components

### 3.1 Signatures: The Building Blocks

Just as traditional programming uses function signatures to define interfaces, DSPy uses signatures to define how your code interacts with language models. These signatures are the foundation of DSPy's declarative programming model.

1. **Basic Signature Syntax**

   DSPy offers multiple ways to define signatures, from simple to complex:

   ```python
   # Simple inline signature for basic tasks
   basic_qa = dspy.Predict("question -> answer")

   # Typed inline signature for better clarity
   typed_qa = dspy.Predict("question: str -> answer: str")
   ```

2. **Class-based Signatures**

   For more complex tasks, class-based signatures provide better structure and type safety:

   ```python
   from typing import Literal

   class EmotionClassifier(dspy.Signature):
       """Classify the emotion in a given text with confidence score."""

       text = dspy.InputField()
       emotion: Literal['joy', 'sadness', 'anger', 'fear', 'surprise'] = dspy.OutputField()
       confidence: float = dspy.OutputField()
   ```

3. **Multi-Field Signatures**

   When you need multiple inputs or outputs:

   ```python
   class DocumentAnalyzer(dspy.Signature):
       """Analyze a document and extract key information."""

       document = dspy.InputField()
       title = dspy.OutputField()
       main_topics = dspy.OutputField(desc="list of main topics")
       summary = dspy.OutputField(desc="brief summary of the content")
   ```

### 3.2 Modules: DSPy's Workhorses

Modules are the core building blocks that implement different prompting strategies and reasoning patterns. DSPy provides several built-in modules:

1. **Predict Module**

   The simplest module for direct LLM predictions:

   ```python
   predictor = dspy.Predict(EmotionClassifier)
   result = predictor(text="I can't believe we won the championship!")
   print(f"Emotion: {result.emotion}, Confidence: {result.confidence}")
   ```

2. **ChainOfThought Module**

   Implements step-by-step reasoning:

   ```python
   class MathProblem(dspy.Signature):
       """Solve math word problems step by step."""
       question = dspy.InputField()
       reasoning = dspy.OutputField(desc="step by step solution")
       answer = dspy.OutputField(desc="final numerical answer")

   solver = dspy.ChainOfThought(MathProblem)
   solution = solver(question="If a train travels 120 km in 2 hours, what is its average speed?")
   ```

3. **RAG (Retrieval-Augmented Generation)**

   RAG combines retrieval of relevant information with language model generation. It's particularly useful when you need to ground LLM responses in specific documents or knowledge bases.

   ```python
   class RAGModule(dspy.Module):
       """A basic RAG implementation."""

       def __init__(self, num_passages=3):
           super().__init__()
           # Set up retriever for finding relevant passages
           self.retrieve = dspy.Retrieve(k=num_passages)
           # Set up generator for producing answers
           self.generate = dspy.ChainOfThought('context, question -> answer')

       def forward(self, question):
           # Retrieve relevant passages
           passages = self.retrieve(question).passages
           # Generate answer using retrieved context
           response = self.generate(context=passages, question=question)
           return response
   ```

4. **ReAct (Reasoning and Acting)**

   ReAct is a powerful pattern for building LLM-powered agents that combines reasoning with action execution in an iterative loop. It enables agents to:

   - **Think**: Reason about the current state and plan next steps
   - **Act**: Execute actions using available tools
   - **Observe**: Process results from actions
   - **Reflect**: Analyze outcomes and adjust strategy

   ```python
   def search_wikipedia(query: str) -> list[str]:
       """Tool for searching Wikipedia."""
       return ["Sample search result"]

   def calculate(expression: str) -> float:
       """Tool for mathematical calculations."""
       return eval(expression)

   # Create a ReAct agent with tools
   react_agent = dspy.ReAct(
       "question -> answer",
       tools=[search_wikipedia, calculate]
   )

   # The agent can now use these tools to answer questions
   result = react_agent(
       question="What is the population of France divided by 2?"
   )
   ```

   **Key Benefits of ReAct:**

   - Enables multi-step problem solving
   - Combines tool use with reasoning
   - Provides traceable decision paths
   - Handles complex tasks requiring multiple operations

### 3.3 Metrics

DSPy provides both built-in metrics and the ability to create custom metrics for evaluating your AI applications. Metrics are functions that quantify the quality of your system's outputs, answering the fundamental question: "What makes outputs good or bad?"

1. **Built-in Metrics**

   DSPy comes with several built-in metrics for common evaluation needs:

   ```python
   # Simple exact match metric
   from dspy.evaluate.metrics import answer_exact_match

   # Check if answer exists in passage
   from dspy.evaluate.metrics import answer_passage_match

   # Semantic evaluation metrics
   from dspy.evaluate.metrics import SemanticF1
   ```

2. **Creating Custom Metrics**

   Metrics in DSPy are functions that take three inputs:

   - `example`: The reference data (ground truth)
   - `pred`: The system's predicted output
   - `trace` (optional): For optimization and debugging

   Here's a simple custom metric:

   ```python
   def validate_answer(example, pred, trace=None):
       """Simple exact match validation."""
       return example.answer.lower() == pred.answer.lower()
   ```

3. **Complex Metrics**

   For more sophisticated evaluation, you can create metrics that assess multiple properties:

   ```python
   def validate_context_and_answer(example, pred, trace=None):
       """Validates both answer accuracy and context relevance."""
       answer_match = example.answer.lower() == pred.answer.lower()
       context_match = any(pred.answer.lower() in c for c in pred.context)

       # For optimization, return average score
       if trace is None:
           return (answer_match + context_match) / 2.0
       # For evaluation, require both conditions
       else:
           return answer_match and context_match
   ```

4. **Advanced Validation with Traces**

   You can use traces to validate intermediate steps and debug your pipeline:

   ```python
   def validate_hops(example, pred, trace=None):
       """Validates reasoning hops in multi-step problems."""
       hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]

       # Check hop length
       if max([len(h) for h in hops]) > 100:
           return False

       # Check for redundant hops
       if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8)
              for idx in range(2, len(hops))):
           return False

       return True
   ```

5. **Using AI Feedback for Evaluation**

   DSPy allows using language models themselves to evaluate outputs along multiple dimensions:

   ```python
   class CompleteAndGrounded(dspy.Module):
       """Evaluates completeness and groundedness of responses."""

       def __init__(self, threshold=0.66):
           self.threshold = threshold
           self.completeness_module = dspy.ChainOfThought(AnswerCompleteness)
           self.groundedness_module = dspy.ChainOfThought(AnswerGroundedness)

       def forward(self, example, pred, trace=None):
           completeness = self.completeness_module(
               question=example.question,
               ground_truth=example.response,
               system_response=pred.response
           )
           groundedness = self.groundedness_module(
               question=example.question,
               retrieved_context=pred.context,
               system_response=pred.response
           )
           score = f1_score(groundedness.groundedness, completeness.completeness)

           return score if trace is None else score >= self.threshold
   ```

6. **Using the Evaluator**

   The `Evaluate` class provides a powerful interface for running evaluations:

   ```python
   from dspy.evaluate import Evaluate

   # Create evaluator with configuration
   evaluator = Evaluate(
       devset=dev_examples,
       num_threads=1,
       display_progress=True,
       display_table=5,
       max_errors=5,
       return_all_scores=False,
       return_outputs=False
   )

   # Run evaluation with your metric
   results = evaluator(predictor, metric=validate_answer)
   ```

7. **Best Practices for Metrics**

   - **Start with simple metrics to establish a baseline.**
   - **Gradually add complexity to address specific evaluation needs.**
   - **Use traces for debugging and optimization.**
   - **Combine multiple metrics for comprehensive evaluation.**
   - **Consider using AI feedback for nuanced assessments.**
   - **Validate both final outputs and intermediate steps when needed.**

### 3.4 Optimization Techniques

DSPy provides several powerful optimizers, each designed for specific use cases. While we cover the most commonly used ones below, DSPy offers many more optimizers like `BootstrapFewShotWithRandomSearch`, `BootstrapFinetune`, `KNNFewShot`, `COPRO`, `BetterTogether`, and others. For a complete list and detailed documentation, visit [DSPy Optimizers Documentation](https://dspy.ai/learn/optimization/optimizers/).

1. **Zero-Shot Optimization with MIPROv2**

   For cases where you want to optimize instructions without using any few-shot examples:

   ```python
   teleprompter = dspy.MIPROv2(
       metric=accuracy_metric,
       auto="light"
   )

   zeroshot_program = teleprompter.compile(
       program.deepcopy(),
       trainset=trainset,
       max_bootstrapped_demos=0,  # No examples
       max_labeled_demos=0        # No examples
   )
   ```

   **When to use:**

   - Need minimal prompt length
   - Want to optimize pure instructions
   - Have strong training data for instruction learning

2. **BootstrapFewShot**

   Best for scenarios with limited training data (5-50 examples) where you want to automatically discover effective few-shot examples.

   ```python
   optimizer = dspy.BootstrapFewShot(
       metric=accuracy_metric,
       max_bootstrapped_demos=4,  # Number of examples to generate
       max_labeled_demos=8,       # Number of real examples to use
       max_rounds=10             # Training iterations
   )

   optimized_predictor = optimizer.compile(
       student=predictor,
       trainset=train_examples
   )
   ```

   **When to use:**

   - Limited training data available
   - Need quick optimization with minimal examples
   - Want to maintain interpretability of prompts

3. **MIPROv2 (Multiprompt Instruction PRoposal Optimizer)**

   Ideal for scenarios with more training data (50+ examples) where you want to optimize both instructions and few-shot examples jointly.

   ```python
   mipro = dspy.MIPROv2(
       metric=accuracy_metric,
       auto="light",  # Choose optimization intensity
       num_candidates=7,  # Number of instruction candidates
       init_temperature=0.5  # Controls instruction diversity
   )

   optimized_program = mipro.compile(
       student=predictor.deepcopy(),
       trainset=trainset,
       max_bootstrapped_demos=3,
       max_labeled_demos=4,
       num_trials=15  # Number of optimization rounds
   )
   ```

   **When to use:**

   - Larger training datasets available
   - Need sophisticated instruction optimization
   - Want to balance exploration and exploitation
   - Can afford longer optimization runs

**Choosing the Right Optimizer:**

| Optimizer         | Training Data Size | Optimization Target     | Time Required | Use Case                             |
| ----------------- | ------------------ | ----------------------- | ------------- | ------------------------------------ |
| Zero-shot MIPROv2 | 50+ examples       | Instructions only       | Hours         | Minimal prompt length needed         |
| BootstrapFewShot  | 5-50 examples      | Few-shot examples       | Minutes       | Quick iteration, limited data        |
| MIPROv2           | 50+ examples       | Instructions & examples | Hours         | Complex tasks, thorough optimization |

## Conclusion

DSPy represents a significant evolution in how we build LLM-powered applications. By moving beyond traditional prompt engineering to a programmatic approach, it addresses many of the challenges that developers face when working with language models.

Key takeaways from this guide:

- **Programmatic Control**: DSPy transforms prompt engineering into a more structured, maintainable programming practice.
- **Modular Design**: The framework's component-based architecture enables building complex systems from simple, reusable parts.
- **Systematic Optimization**: Built-in optimization tools help improve system performance automatically.
- **Production-Ready**: DSPy's approach makes it easier to build reliable, testable AI applications.

Whether you're building a simple question-answering system or a complex AI application, DSPy provides the tools and patterns needed to create robust solutions. As the field of AI continues to evolve, frameworks like DSPy will become increasingly important for developing production-grade LLM applications.

Remember: The goal isn't just to make LLMs work – it's to make them work reliably, maintainably, and at scale. DSPy helps achieve this by bringing software engineering best practices to LLM development.

Ready to start building? Check out the [DSPy documentation](https://dspy.ai) and join the growing community of developers building the future of AI applications.
